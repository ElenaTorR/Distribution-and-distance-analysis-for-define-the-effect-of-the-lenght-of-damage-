{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as poff\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import scipy.io.wavfile\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib import pyplot\n",
    "import datetime\n",
    "from dateutil.relativedelta import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance_matrix\n",
    "from past.builtins import xrange\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import wasserstein_distance\n",
    "import warnings\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE 2 DATASETS WITH DATA BEFORE AND DURING BEARING DAMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic_a ={}\n",
    "df_dic_b ={}\n",
    "df_dic ={}\n",
    "data_dir = r'S:\\Analytics\\TemporaryThings\\Bearing_lenght_of_damage\\condition_data'\n",
    "for temp in os.listdir(data_dir):\n",
    "    tmp = temp.split('Export_')[-1].split('.csv')[0]\n",
    "    file = join(data_dir,temp)\n",
    "    df_dic[tmp] = pd.read_csv(file,index_col = False,\n",
    "        parse_dates = ['UTC Time Date'],usecols = ['UTC Time Date','BHI','WHI','Speed','RMS X','RMS Y','RMS Z','Peak X','Peak Y','Peak Z'])\n",
    "    df_dic_a[tmp] = df_dic[tmp].loc[(df_dic[tmp]['UTC Time Date']>= df_dic[tmp]['UTC Time Date'].max()- datetime.timedelta(days=30))]\n",
    "    df_dic_b[tmp] = df_dic[tmp].loc[(df_dic[tmp]['UTC Time Date']< df_dic[tmp]['UTC Time Date'].max()- datetime.timedelta(days=90))]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pd.concat(df_dic_a)\n",
    "df_b = pd.concat(df_dic_b)\n",
    "df_a = df_a.reset_index().rename(columns ={'level_0':'Wheel'}).drop(['level_1'],axis=1)\n",
    "df_a = df_a.loc[(df_a['Speed']>60) & (df_a['Peak X']> 0) & (df_a['Peak Y']> 0) & (df_a['Peak Z']>0) & (df_a['RMS X']>0) & (df_a['RMS Y']>0)  & (df_a['RMS Z']>0)]\n",
    "df_b = df_b.reset_index().rename(columns ={'level_0':'Wheel'}).drop(['level_1'],axis=1)\n",
    "df_b = df_b.loc[(df_b['Speed']>60) & (df_b['Peak X']> 0) & (df_b['Peak Y']> 0) & (df_b['Peak Z']>0) & (df_b['RMS X']>0) & (df_b['RMS Y']>0)  & (df_b['RMS Z']>0)]                                                                                                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE TRAIN AND TEST SET FOR DATAFRAME WITH DATA BEFORE AND DURING BEARING DAMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test set\n",
    "\n",
    "train_a, test_a = train_test_split(df_a['Wheel'].unique(),test_size=0.2)\n",
    "df_train_l_a = {}\n",
    "df_test_l_a = {}\n",
    "for t in train_a:\n",
    "    df_train_l_a[t] = df_a.loc[df_a['Wheel']==t]\n",
    "for te in test_a:\n",
    "    df_test_l_a[te] = df_a.loc[df_a['Wheel']==te]\n",
    "\n",
    "df_train_s_a = pd.concat(df_train_l_a).reset_index().drop(['level_0','level_1'],axis=1) \n",
    "df_train_s_a = pd.concat([pd.DataFrame(df_train_s_a.Wheel.str.split('_').tolist(),columns = ['Customer ID','Unit','Vehicle','Wheel']),df_train_s_a.iloc[:,1:]],axis = 1,sort = False)\n",
    "df_test_s_a = pd.concat(df_test_l_a).reset_index().drop(['level_0','level_1'],axis=1)\n",
    "df_test_s_a = pd.concat([pd.DataFrame(df_test_s_a.Wheel.str.split('_').tolist(),columns = ['Customer ID','Unit','Vehicle','Wheel']),df_test_s_a.iloc[:,1:]],axis = 1,sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_b, test_b = train_test_split(df_b['Wheel'].unique(),test_size=0.2)\n",
    "df_train_l_b = {}\n",
    "df_test_l_b = {}\n",
    "for t in train_b:\n",
    "    df_train_l_b[t] = df_b.loc[df_b['Wheel']==t]\n",
    "for te in test_b:\n",
    "    df_test_l_b[te] = df_b.loc[df_b['Wheel']==te]\n",
    "\n",
    "df_train_s_b = pd.concat(df_train_l_b).reset_index().drop(['level_0','level_1'],axis=1) \n",
    "df_train_s_b = pd.concat([pd.DataFrame(df_train_s_b.Wheel.str.split('_').tolist(),columns = ['Customer ID','Unit','Vehicle','Wheel']),df_train_s_b.iloc[:,1:]],axis = 1,sort = False)\n",
    "#df_train_s_b['ID'] = df_train_s_b[['Customer ID','Unit','Vehicle','Wheel']].agg('_'.join, axis=1)\n",
    "df_test_s_b = pd.concat(df_test_l_b).reset_index().drop(['level_0','level_1'],axis=1)\n",
    "#df_test_s_b['ID'] = df_test_s_b[['Customer ID','Unit','Vehicle','Wheel']].agg('_'.join, axis=1)\n",
    "df_test_s_b = pd.concat([pd.DataFrame(df_test_s_b.Wheel.str.split('_').tolist(),columns = ['Customer ID','Unit','Vehicle','Wheel']),df_test_s_b.iloc[:,1:]],axis = 1,sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD A COLUMN WITH LEVEL OF DAMAGE (CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_len = pd.read_csv(r'S:\\Analytics\\TemporaryThings\\Bearing_lenght_of_damage\\length_of_bearing_damage\\length_of_damage.csv',dtype = {'Fleet ID': str, 'Unit': str,\n",
    "    'Vehicle': str},usecols = ['Fleet ID','Confirmation date','Unit','Vehicle','Wheel','Damage length(mm)'],parse_dates = ['Confirmation date']).dropna()\n",
    "\n",
    "df_len.loc[(df_len['Damage length(mm)'] <= 30),'Damage Cat']=0\n",
    "df_len.loc[(df_len['Damage length(mm)'] > 30) & (df_len['Damage length(mm)'] <=80),'Damage Cat']=1\n",
    "df_len.loc[(df_len['Damage length(mm)'] > 80),'Damage Cat']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_a = pd.merge(df_train_s_a,df_len,on=['Unit','Wheel']).drop(['Vehicle_x'],axis = 1).rename(columns={\"Vehicle_y\": \"Vehicle\"}).dropna()\n",
    "df_train_a['ID'] = df_train_a[['Customer ID', 'Unit','Vehicle','Wheel']].apply(lambda x: '_'.join(x), axis=1)\n",
    "df_test_a = pd.merge(df_test_s_a,df_len,on=['Unit','Wheel']).drop(['Vehicle_x'],axis = 1).rename(columns={\"Vehicle_y\": \"Vehicle\"}).dropna()\n",
    "df_test_a['ID'] = df_test_a[['Customer ID', 'Unit','Vehicle','Wheel']].apply(lambda x: '_'.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_b = pd.merge(df_train_s_b,df_len,on=['Unit','Wheel']).drop(['Vehicle_x'],axis = 1).rename(columns={\"Vehicle_y\": \"Vehicle\"}).dropna()\n",
    "df_train_b['ID'] = df_train_b[['Customer ID', 'Unit','Vehicle','Wheel']].apply(lambda x: '_'.join(x), axis=1)\n",
    "df_test_b = pd.merge(df_test_s_b,df_len,on=['Unit','Wheel']).drop(['Vehicle_x'],axis = 1).rename(columns={\"Vehicle_y\": \"Vehicle\"}).dropna()\n",
    "df_test_b['ID'] = df_test_b[['Customer ID', 'Unit','Vehicle','Wheel']].apply(lambda x: '_'.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COUNT FOR EACH FLEET THE NUMBER OF DAMAGE FOR EACH CATEGORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = []\n",
    "for (f,ins),vals in df_train_a.groupby(['Fleet ID','Damage Cat']):\n",
    "    count.append(df_train_a.loc[(df_train_a['Fleet ID'] == f) & (df_train_a['Damage Cat'] == ins)].groupby(['Fleet ID','Damage Cat'])[['Wheel']].nunique())\n",
    "count = pd.concat(count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHANGE OF VIBRATIONS DISTRIBUTION WHEN THE DAMAGE IS GETS WORST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in ['170','171','375','376','377','465']:\n",
    "    df_q = df_train_a.loc[(df_train_a['Fleet ID'] == f)]\n",
    "    df_q = df_q.set_index('UTC Time Date').groupby(pd.Grouper(freq='D')).quantile(0.85)\n",
    "    fig, ax = plt.subplots(1, 6, figsize=(20,5))\n",
    "    for num,p in enumerate(['Peak X','Peak Y','Peak Z','RMS X','RMS Y','RMS Z']):\n",
    "        sns.distplot(df_q.loc[(df_q['Damage Cat'] == 0)][p], hist=False, rug=False,ax = ax[num],label = 'light')\n",
    "        sns.distplot(df_q.loc[(df_q['Damage Cat'] == 1)][p], hist=False, rug=False,ax = ax[num],label = 'middle')\n",
    "        sns.distplot(df_q.loc[(df_q['Damage Cat'] == 2)][p], hist=False, rug=False,ax = ax[num],label = 'high')\n",
    "        plt.title(''.join([f,' 85th Percentile']),fontsize = 20)\n",
    "        ax[num].legend(fontsize = 15)\n",
    "        ax[num].tick_params(axis=\"x\", labelsize=20)\n",
    "        ax[num].tick_params(axis=\"y\", labelsize=20)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        ax[num].set_xlabel(p, fontsize=20)\n",
    "        plt.savefig(''.join(['C:\\\\Users\\\\ElenaR\\\\Documents\\\\Bearing_lenght_of_damage\\\\output\\\\',f,' 85th Percentile.png']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTANCE MATRIX OF AVERAGING STNDARDIZE VIBRATIONS INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_low = distance_matrix(preprocessing.scale(pd.DataFrame(df_0_m_a).T),preprocessing.scale(pd.DataFrame(df_0_m_a).T))\n",
    "distance_middle = distance_matrix(preprocessing.scale(pd.DataFrame(df_1_m_a).T),preprocessing.scale(pd.DataFrame(df_1_m_a).T))\n",
    "distance_high = distance_matrix(preprocessing.scale(pd.DataFrame(df_2_m_a).T),preprocessing.scale(pd.DataFrame(df_2_m_a).T))\n",
    "\n",
    "for n,fleet in enumerate(df_train_a['Fleet ID'].unique()):\n",
    "    distance_low  = (pd.DataFrame(distance_low)).rename(columns = {n : fleet},index = {n : fleet})\n",
    "    distance_middle  = (pd.DataFrame(distance_middle)).rename(columns = {n : fleet},index = {n : fleet})\n",
    "    distance_high  = (pd.DataFrame(distance_high)).rename(columns = {n : fleet},index = {n : fleet} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(50,50))\n",
    "sns.heatmap(distance_low.T, square=True, annot=True,annot_kws={\"size\": 30},cmap='coolwarm', cbar=False,ax = ax1)\n",
    "ax1.set_title('Distance between fleets (low damage)',fontsize= 30)\n",
    "sns.heatmap(distance_middle.T, square=True, annot=True,annot_kws={\"size\": 30},cmap='coolwarm', cbar=False,ax = ax2)\n",
    "ax2.set_title('Distance between fleets (middle damage)',fontsize= 30)\n",
    "sns.heatmap(distance_high.T, square=True, annot=True,annot_kws={\"size\": 30},cmap='coolwarm', cbar=False,ax = ax3)\n",
    "ax3.set_title('Distance between fleets (high damage)',fontsize= 30)\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.tick_params(axis=\"x\", labelsize=30)\n",
    "    ax.tick_params(axis=\"y\", labelsize=30)\n",
    "plt.savefig(''.join(['C:\\\\Users\\\\ElenaR\\\\Documents\\\\Bearing_lenght_of_damage\\\\output\\\\distance.png']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARE VIBRATIONS BEFORE AND AFTER DAMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = 'Peak X'\n",
    "for fleet in ['170','171','375','376','377','465']:\n",
    "    df1 = df_train_a.loc[df_train_a['Fleet ID']==fleet].groupby(['Damage Cat',pd.Grouper(key='UTC Time Date', freq='D')])[['Peak X','Peak Y','Peak Z','RMS X','RMS Y','RMS Z','Damage length(mm)']].mean()\n",
    "    df2 = df_train_b.loc[df_train_b['Fleet ID']==fleet].groupby(['Damage Cat',pd.Grouper(key='UTC Time Date', freq='D')])[['Peak X','Peak Y','Peak Z','RMS X','RMS Y','RMS Z','Damage length(mm)']].mean()\n",
    "\n",
    "    for num,p in enumerate(['Peak X','Peak Y','Peak Z','RMS X','RMS Y','RMS Z']):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(40,10))\n",
    "        sns.distplot(df1.loc[df1.index.get_level_values(0)==0][p],hist=False,label = 'light damage',color = 'red',ax = ax[0])\n",
    "        sns.distplot(df2.loc[df2.index.get_level_values(0)==0][p],hist=False,label = 'No light damage',color = 'blue',ax = ax[0])\n",
    "        ax[0].legend(fontsize = 30)\n",
    "        sns.distplot(df1.loc[df1.index.get_level_values(0)==1][p],hist=False,label = 'middle damage',color = 'red',ax = ax[1])\n",
    "        sns.distplot(df2.loc[df2.index.get_level_values(0)==1][p],hist=False,label = 'No middle damage',color = 'blue',ax = ax[1])\n",
    "        ax[1].legend(fontsize = 30)\n",
    "        sns.distplot(df1.loc[df1.index.get_level_values(0)==2][p],hist=False,label = 'high damage',color = 'red',ax = ax[2])\n",
    "        sns.distplot(df2.loc[df2.index.get_level_values(0)==2][p],hist=False,label = 'No high damage',color = 'blue',ax = ax[2])\n",
    "        ax[2].legend(fontsize = 30)\n",
    "\n",
    "        for axs in [ax[0],ax[1],ax[2]]:\n",
    "            axs.set_title(' '.join(['Vibrations distribution ',fleet]),fontweight = 'bold',fontsize = 20)\n",
    "            axs.tick_params(axis=\"x\", labelsize=30)\n",
    "            axs.tick_params(axis=\"y\", labelsize=30)\n",
    "            axs.set_xlabel(p, fontsize=30)\n",
    "        fig.savefig(''.join(['C:\\\\Users\\\\ElenaR\\\\Documents\\\\Bearing_lenght_of_damage\\\\output\\\\',fleet,'_',p,'.png']))\n",
    "    \n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distribution distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []\n",
    "for fleet in ['170','171','375','376','377','465']:\n",
    "    da = df_train_a.loc[(df_train_a['Fleet ID'] == fleet)]\n",
    "    db = df_train_b.loc[(df_train_b['Fleet ID'] == fleet)]\n",
    "    for p in ['Peak X','Peak Y','Peak Z','RMS X','RMS Y','RMS Z']:\n",
    "        w.append({'Distribution distance': wasserstein_distance(db[p],da[p]),'Parameter' : p,'Fleet' : fleet, 'Light damage average': da.loc[(da['Damage Cat'] == 0)]['Damage length(mm)'].mean() ,'Middle damage average': da.loc[(da['Damage Cat'] == 1)]['Damage length(mm)'].mean()})\n",
    "        \n",
    "\n",
    "w = pd.DataFrame(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCATTER PLOT LENGHT OF DAMAGE AGAINST MAX BHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = 20\n",
    "index = 'BHI'\n",
    "plt.figure(figsize = (20,14))\n",
    "for fleet in df_train_a['Fleet ID'].unique():\n",
    "    plt.scatter(df_train_a.loc[df_train_a['Fleet ID']==fleet].groupby(['Unit','Wheel'])['Damage length(mm)'].max(),df_train_a.loc[df_train_a['Fleet ID']==fleet].groupby(['Unit','Wheel'])[index].max(),s = 60)\n",
    "plt.legend(df_train_a['Fleet ID'].unique(),fontsize = f)\n",
    "plt.title('max BHI vs lenght of damage',fontsize = f)\n",
    "plt.xticks(fontsize = f)\n",
    "plt.yticks(fontsize = f)\n",
    "plt.xlabel('length of damage',fontsize = f)\n",
    "plt.ylabel('max BHI',fontsize = f)\n",
    "plt.savefig(r'C:\\Users\\ElenaR\\Documents\\Bearing_lenght_of_damage\\output\\max BHI vs lenght of damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
